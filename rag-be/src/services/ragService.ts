import { ChatOpenAI } from "@langchain/openai";
import { initializeVectorStore } from "./vectorStore";
import { logger } from "../utils/logger";

export interface SourceDocument {
  disease_name: string;
  content: string;
  section?: string;
}

export interface StreamResult {
  type: 'sources' | 'chunk' | 'done';
  sources?: SourceDocument[];
  chunk?: string;
}

/**
 * Default RAG Mode - ì§ì ‘ ê²€ìƒ‰ ë°©ì‹ (Streaming)
 */
export async function* generateAnswerStream(query: string): AsyncGenerator<StreamResult> {
  // 1. VectorStore ì´ˆê¸°í™”
  const vectorStore = await initializeVectorStore();

  // 2. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
  logger.info("ğŸ” Searching for relevant documents...");
  const retrievedDocs = await vectorStore.similaritySearch(query, 3);
  logger.info(`âœ“ Found ${retrievedDocs.length} documents`);

  // 3. ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ë¥¼ ë¨¼ì € ì „ì†¡
  const sources: SourceDocument[] = retrievedDocs.map((doc) => ({
    disease_name: doc.metadata.disease_name || "Unknown",
    content: doc.pageContent.substring(0, 200) + "...", // ë¯¸ë¦¬ë³´ê¸°
    section: doc.metadata.section,
  }));

  yield { type: 'sources', sources };

  // 4. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
  const context = retrievedDocs
    .map(
      (doc, idx) =>
        `[ë¬¸ì„œ ${idx + 1}]\nì¶œì²˜: ${doc.metadata.disease_name || "Unknown"}\në‚´ìš©: ${doc.pageContent}`
    )
    .join("\n\n");

  // 5. LLM ì´ˆê¸°í™”
  const llm = new ChatOpenAI({
    openAIApiKey: process.env.OPENAI_API_KEY,
    modelName: "gpt-4o-mini",
    temperature: 0,
    streaming: true,
  });

  // 6. í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
  const prompt = `ë‹¹ì‹ ì€ ì˜ë£Œ ì •ë³´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

# ë‹µë³€ ì§€ì¹¨
- ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- ì˜í•™ì ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”
- ê²€ìƒ‰ ê²°ê³¼ì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ë¬¸ì„œì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”
- í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•˜ì„¸ìš”
- ë‹µë³€ ëì— ì°¸ê³ í•œ ë¬¸ì„œì˜ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”

# ê²€ìƒ‰ëœ ë¬¸ì„œ
${context}

# ì‚¬ìš©ì ì§ˆë¬¸
${query}

# ë‹µë³€`;

  logger.info("â³ Generating streaming answer...");

  const stream = await llm.stream(prompt);

  for await (const chunk of stream) {
    const content = chunk.content;
    if (typeof content === 'string' && content) {
      yield { type: 'chunk', chunk: content };
    }
  }

  yield { type: 'done' };

  logger.info("âœ“ Answer generation completed");
}
