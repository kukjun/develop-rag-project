import { ChatOpenAI } from "@langchain/openai";
import { OpenAIEmbeddings } from "@langchain/openai";
import { PGVectorStore } from "@langchain/community/vectorstores/pgvector";
import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { tool } from "@langchain/core/tools";
import { z } from "zod";
import { logger } from "../utils/logger";
import { StreamResult, SourceDocument } from "./ragService";

// ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì €ì¥í•  ë³€ìˆ˜ (Agent ë‚´ë¶€ì—ì„œ ì ‘ê·¼í•˜ê¸° ìœ„í•´)
let retrievedDocuments: SourceDocument[] = [];

/**
 * Retrieve Tool ìƒì„±
 */
async function createRetrieveTool() {
  const embeddings = new OpenAIEmbeddings({
    openAIApiKey: process.env.OPENAI_API_KEY,
    modelName: "text-embedding-3-small",
  });

  const config = {
    postgresConnectionOptions: {
      type: "postgres" as const,
      host: process.env.PGVECTOR_HOST || "localhost",
      port: parseInt(process.env.PGVECTOR_PORT || "5433"),
      user: process.env.PGVECTOR_USER || "postgres",
      password: process.env.PGVECTOR_PASSWORD || "password",
      database: process.env.PGVECTOR_DATABASE || "medical_rag",
    },
    tableName: "medical_rag_embeddings",
    columns: {
      idColumnName: "id",
      vectorColumnName: "embedding",
      contentColumnName: "content",
      metadataColumnName: "metadata",
    },
  };

  logger.info("Connecting to PGVector for tool...");
  const vectorStore = await PGVectorStore.initialize(embeddings, config);
  logger.info("âœ“ PGVector connected for tool");

  const retrieveSchema = z.object({ query: z.string() });

  // @ts-ignore
  const retrieve = tool(
    async ({ query }) => {
      const retrievedDocs = await vectorStore.similaritySearch(query, 3);

      // ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ ì €ì¥
      retrievedDocuments = retrievedDocs.map((doc) => ({
        disease_name: doc.metadata.disease_name || "Unknown",
        content: doc.pageContent.substring(0, 200) + "...",
        section: doc.metadata.section,
      }));

      const serialized = retrievedDocs
        .map(
          (doc) =>
            `Source: ${doc.metadata.disease_name || "Unknown"}\nContent: ${doc.pageContent}`
        )
        .join("\n\n");
      return [serialized, retrievedDocs];
    },
    {
      name: "retrieve_medical_info",
      description: "ì˜ë£Œ ì •ë³´ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
      schema: retrieveSchema,
      responseFormat: "content_and_artifact",
    }
  );

  return retrieve;
}

/**
 * AI Agent Mode - Tool ì‚¬ìš© ë°©ì‹ (Streaming)
 */
export async function* generateAnswerWithAgentStream(query: string): AsyncGenerator<StreamResult> {
  logger.info("ğŸ¤– Creating AI Agent with retrieve tool...");

  // ì´ˆê¸°í™”
  retrievedDocuments = [];

  // 1. Retrieve Tool ìƒì„±
  const retrieveTool = await createRetrieveTool();
  logger.info("âœ“ Tool created");

  // 2. System prompt
  const systemPrompt = `ë‹¹ì‹ ì€ ì˜ë£Œ ì •ë³´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

# ë‹µë³€ ì§€ì¹¨
- ì˜í•™ ê´€ë ¨ëœ ì§ˆë¬¸ì´ ì˜¤ë©´, ë°˜ë“œì‹œ retrieve_medical_info ë„êµ¬ë¡œ ê´€ë ¨ ë¬¸ì„œë¥¼ ë¨¼ì € ê²€ìƒ‰í•˜ì„¸ìš”.
- ì˜í•™ê³¼ ê´€ë ¨ë˜ì§€ ì•Šì€ ì§ˆë¬¸ì€ ê·¸ëƒ¥ ë‹µë³€í•˜ì„¸ìš”.
- ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- ì˜í•™ì ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”
- ê²€ìƒ‰ ê²°ê³¼ì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ë¬¸ì„œì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”
- í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•˜ì„¸ìš”
- ê²€ìƒ‰í•œ ë¬¸ì„œì˜ ì¶œì²˜ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”`;

  // 3. LLM ì´ˆê¸°í™”
  const llm = new ChatOpenAI({
    openAIApiKey: process.env.OPENAI_API_KEY,
    modelName: "gpt-4o-mini",
    temperature: 0,
    streaming: true,
  });

  // 4. React Agent ìƒì„±
  logger.info("â³ Creating React Agent...");
  // @ts-ignore
  const agent = createReactAgent({
    llm,
    tools: [retrieveTool],
    messageModifier: systemPrompt,
  });
  logger.info("âœ“ Agent created");

  // 5. Agent ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°)
  logger.info("ğŸ”„ Agent is processing your query...");

  const stream = await agent.stream(
    { messages: [{ role: "user", content: query }] },
    { streamMode: "values" }
  );

  let sourcesEmitted = false;

  for await (const step of stream) {
    const lastMessage = step.messages[step.messages.length - 1];
    const messageType = lastMessage._getType();

    // Toolì´ ì‹¤í–‰ëœ í›„ sourcesë¥¼ ì „ì†¡
    if (messageType === "tool" && !sourcesEmitted && retrievedDocuments.length > 0) {
      yield { type: 'sources', sources: retrievedDocuments };
      sourcesEmitted = true;
    }

    // AIì˜ ìµœì¢… ì‘ë‹µë§Œ ìŠ¤íŠ¸ë¦¬ë° (tool_callsê°€ ì—†ëŠ” ê²½ìš°)
    if (
      messageType === "ai" &&
      (!lastMessage.tool_calls || lastMessage.tool_calls.length === 0)
    ) {
      const content =
        typeof lastMessage.content === "string"
          ? lastMessage.content
          : Array.isArray(lastMessage.content)
          ? lastMessage.content.map((c: any) => c.text || c).join("")
          : JSON.stringify(lastMessage.content);

      if (content && content.length > 0) {
        yield { type: 'chunk', chunk: content };
      }
    }
  }

  yield { type: 'done' };

  logger.info("âœ“ Agent answer generation completed");
}
