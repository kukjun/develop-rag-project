import "dotenv/config";
import * as fs from "fs/promises";
import * as path from "path";
import { OpenAIEmbeddings } from "@langchain/openai";
import { Document } from "@langchain/core/documents";
import { PGVectorStore } from "@langchain/community/vectorstores/pgvector";
import { parseMarkdownToDocuments } from "./utils/md-parser";
import { generateChunks } from "./utils/chunk-generator";
import { Chunk } from "./types/chunk";
import { logger } from "./utils/logger";

/**
 * MD íŒŒì¼ì—ì„œ Chunks ìƒì„±
 */
async function processMarkdownFile(filePath: string): Promise<Chunk[]> {
  logger.info(`Processing: ${filePath}`);

  // 1. íŒŒì¼ ì½ê¸°
  const content = await fs.readFile(filePath, "utf-8");

  // 2. Frontmatter + Documents íŒŒì‹± (LangChain MarkdownTextSplitter ì‚¬ìš©)
  const { frontmatter, documents } = await parseMarkdownToDocuments(content);

  // 3. Chunks ìƒì„±
  const chunks = generateChunks(frontmatter, documents);

  logger.info(`  Generated ${chunks.length} chunks`);
  return chunks;
}

/**
 * ëª¨ë“  MD íŒŒì¼ ì²˜ë¦¬
 */
async function processAllMarkdownFiles(): Promise<Chunk[]> {
  const dataDir = path.join(
    __dirname,
    "..",
    "..",
    "data",
    "processed",
    "cancers"
  );
  const files = await fs.readdir(dataDir);
  const mdFiles = files.filter((f) => f.endsWith(".md"));

  logger.info(`Found ${mdFiles.length} markdown files\n`);

  const allChunks: Chunk[] = [];

  for (const file of mdFiles) {
    const filePath = path.join(dataDir, file);
    const chunks = await processMarkdownFile(filePath);
    allChunks.push(...chunks);
  }

  return allChunks;
}

/**
 * Chunksë¥¼ PGVectorì— ì‚½ì…
 */
async function insertToVectorDB(chunks: Chunk[]): Promise<void> {
  logger.info(`\nInserting ${chunks.length} chunks to PGVector...`);

  // OpenAI Embeddings ì´ˆê¸°í™”
  logger.info("Initializing OpenAI Embeddings...");
  const embeddings = new OpenAIEmbeddings({
    openAIApiKey: process.env.OPENAI_API_KEY,
    modelName: "text-embedding-3-small",
  });
  logger.info("âœ“ OpenAI Embeddings initialized");

  // Chunksë¥¼ LangChain Documentë¡œ ë³€í™˜
  logger.info("Converting chunks to LangChain Documents...");
  const documents = chunks.map(
    (chunk) =>
      new Document({
        pageContent: chunk.content,
        metadata: chunk.metadata,
      })
  );
  logger.info(`âœ“ Converted ${documents.length} documents`);

  // PGVector ì„¤ì •
  const config = {
    postgresConnectionOptions: {
      type: "postgres",
      host: process.env.PGVECTOR_HOST || "localhost",
      port: parseInt(process.env.PGVECTOR_PORT || "5433"),
      user: process.env.PGVECTOR_USER || "postgres",
      password: process.env.PGVECTOR_PASSWORD || "password",
      database: process.env.PGVECTOR_DATABASE || "medical_rag",
    },
    tableName: "medical_rag_embeddings",
    columns: {
      idColumnName: "id",
      vectorColumnName: "embedding",
      contentColumnName: "content",
      metadataColumnName: "metadata",
    },
  };

  // PGVectorì— ë¬¸ì„œ ì‚½ì…
  logger.info("Creating PGVector store and inserting documents...");
  await PGVectorStore.fromDocuments(documents, embeddings, config);

  logger.info("âœ… Successfully inserted all chunks to PGVector");

  // ìƒ˜í”Œ chunk ì¶œë ¥
  logger.info("\nSample chunks with metadata:");
  chunks.slice(0, 3).forEach((chunk, i) => {
    logger.info(`\n${"=".repeat(60)}`);
    logger.info(`[${i + 1}] Chunk ID: ${chunk.id}`);
    logger.info(`${"=".repeat(60)}`);

    logger.info("\nğŸ“‹ Metadata:");
    logger.info(JSON.stringify(chunk.metadata, null, 2));

    logger.info("\nğŸ“„ Content Preview:");
    logger.info(chunk.content.substring(0, 200) + "...\n");
  });
}

/**
 * ë©”ì¸ ì‹¤í–‰
 */
async function main() {
  try {
    // 1. ëª¨ë“  MD íŒŒì¼ ì²˜ë¦¬
    const chunks = await processAllMarkdownFiles();

    logger.info(`\nTotal chunks generated: ${chunks.length}`);

    // 2. Vector DBì— ì‚½ì…
    await insertToVectorDB(chunks);

    logger.info("\nâœ… Insert completed!");
  } catch (error) {
    logger.error("Error:", error);
    process.exit(1);
  }
}

// ì‹¤í–‰
main();
